# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IiITFrMdHS4wrwmIGK0-RfTOkMwJRK0P
"""

!pip install eefolium tensorflow rasterio

"""Memory-Optimized Wildfire Classification"""
import os
import shutil
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers, applications
import numpy as np
from tensorflow.keras.preprocessing import image_dataset_from_directory
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, classification_report, confusion_matrix

# Configuration
DATA_DIR = "/content/drive/MyDrive/FIRMS_Data"
SPLIT_RATIO = (0.7, 0.2, 0.1)  # Train, Val, Test
SEED = 42

# Create destination folders
for split in ['train', 'val', 'test']:
    for cls in ['fire', 'no_fire']:
        os.makedirs(os.path.join(DATA_DIR, split, cls), exist_ok=True)

# Process each class
for class_name in ['fire', 'no_fire']:
    src_dir = os.path.join(DATA_DIR, class_name)
    files = [f for f in os.listdir(src_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]

    # First split: train+val vs test
    train_val_files, test_files = train_test_split(
        files,
        test_size=SPLIT_RATIO[2],
        random_state=SEED
    )

    # Second split: train vs val
    train_files, val_files = train_test_split(
        train_val_files,
        test_size=SPLIT_RATIO[1]/(SPLIT_RATIO[0]+SPLIT_RATIO[1]),
        random_state=SEED
    )

    # Copy files to destination folders
    for f in train_files:
        shutil.copy(
            os.path.join(src_dir, f),
            os.path.join(DATA_DIR, 'train', class_name, f)
        )

    for f in val_files:
        shutil.copy(
            os.path.join(src_dir, f),
            os.path.join(DATA_DIR, 'val', class_name, f)
        )

    for f in test_files:
        shutil.copy(
            os.path.join(src_dir, f),
            os.path.join(DATA_DIR, 'test', class_name, f)
        )

print("Dataset splitting completed!")

# Configuration
DATA_DIR = "/content/drive/MyDrive/FIRMS_Data"
IMG_SIZE = (256, 256)  # Reduced from original to save memory
BATCH_SIZE = 32

# Elastic Net regularization parameters
L1 = 0.0005
L2 = 0.0005

# Load datasets
train_ds = image_dataset_from_directory(
    f'{DATA_DIR}/train',
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='binary'
)

val_ds = image_dataset_from_directory(
    f'{DATA_DIR}/val',
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='binary'
)

test_ds = image_dataset_from_directory(
    f'{DATA_DIR}/test',
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='binary'
)

# Calculate class weights
fire_count = len(os.listdir(f'{DATA_DIR}/train/fire'))
no_fire_count = len(os.listdir(f'{DATA_DIR}/train/no_fire'))
total = fire_count + no_fire_count
class_weight = {
    0: (1 / no_fire_count) * (total / 2.0),  # no_fire
    1: (1 / fire_count) * (total / 2.0)       # fire
}

# Simple model with Elastic Net regularization
model = models.Sequential([
    # Basic augmentation
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),

    # Base network
    layers.Rescaling(1./255),
    layers.Conv2D(32, (3, 3), activation='relu',
                 kernel_regularizer=regularizers.l1_l2(l1=L1, l2=L2)),
    layers.MaxPooling2D(2, 2),
    layers.Dropout(0.3),

    layers.Conv2D(64, (3, 3), activation='relu',
                 kernel_regularizer=regularizers.l1_l2(l1=L1, l2=L2)),
    layers.MaxPooling2D(2, 2),
    layers.Dropout(0.3),

    layers.Flatten(),
    layers.Dense(64, activation='relu',
                kernel_regularizer=regularizers.l1_l2(l1=L1, l2=L2)),
    layers.Dropout(0.5),

    layers.Dense(1, activation='sigmoid')
])

# Custom learning rate with decay
optimizer = tf.keras.optimizers.Adam(
    learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-3,
        decay_steps=1000,
        decay_rate=0.9
    )
)

model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=[
        'accuracy',
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall'),
        tf.keras.metrics.AUC(name='auc')
    ]
)

# Callbacks
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        patience=5,
        monitor='val_auc',
        mode='max',
        restore_best_weights=True
    ),
    tf.keras.callbacks.ModelCheckpoint(
        'best_model.keras',
        save_best_only=True,
        monitor='val_auc'
    )
]

# Train
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=50,
    class_weight=class_weight,
    callbacks=callbacks
)

# Evaluate on test set
test_results = model.evaluate(test_ds)
print(f"\nTest Performance:\n"
      f"Loss: {test_results[0]:.4f}\n"
      f"Accuracy: {test_results[1]:.4f}\n"
      f"Precision: {test_results[2]:.4f}\n"
      f"Recall: {test_results[3]:.4f}\n"
      f"AUC: {test_results[4]:.4f}")



# Configuration
DATA_DIR = "/content/drive/MyDrive/FIRMS_Data"
IMG_SIZE = (224, 224)  # Standard size for MobileNetV2
BATCH_SIZE = 32

# Load datasets with caching/prefetch
train_ds = image_dataset_from_directory(
    f'{DATA_DIR}/train',
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='binary'
).cache().prefetch(tf.data.AUTOTUNE)

val_ds = image_dataset_from_directory(
    f'{DATA_DIR}/val',
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='binary'
).cache().prefetch(tf.data.AUTOTUNE)

# Calculate class weights
fire_count = len(os.listdir(f'{DATA_DIR}/train/fire'))
no_fire_count = len(os.listdir(f'{DATA_DIR}/train/no_fire'))
total = fire_count + no_fire_count
class_weight = {0: 1/(no_fire_count/total), 1: 1/(fire_count/total)}

# Focal loss implementation
def focal_loss(gamma=2.0, alpha=0.25):
    def loss(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        ce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
        pt = tf.exp(-ce)
        return tf.reduce_mean(alpha * (1-pt)**gamma * ce)
    return loss

# Model construction
base_model = applications.MobileNetV2(
    input_shape=(*IMG_SIZE, 3),
    include_top=False,
    weights='imagenet'
)

model = models.Sequential([
    # Strategic augmentation
    layers.RandomFlip('horizontal_and_vertical'),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2),

    # Preprocessing
    layers.Rescaling(1./127.5, offset=-1),  # MobileNet scaling

    # Base model
    base_model,

    # Custom head
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

# Freeze base model initially
base_model.trainable = False

# Initial compilation
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss=focal_loss(),
    metrics=[
        tf.keras.metrics.AUC(name='auc'),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall')
    ]
)

# Phase 1: Train the head
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    class_weight=class_weight
)

# Phase 2: Fine-tune entire model
base_model.trainable = True
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss=focal_loss(),
    metrics=[
        tf.keras.metrics.AUC(name='auc'),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall')
    ]
)

history_fine = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    class_weight=class_weight,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_auc',
            patience=3,
            restore_best_weights=True
        )
    ]
)

# Post-training threshold optimization
val_preds = model.predict(val_ds)
val_labels = np.concatenate([y for x, y in val_ds], axis=0)

thresholds = np.linspace(0.1, 0.5, 20)
best_f1 = 0
best_threshold = 0.5

for thresh in thresholds:
    preds = (val_preds > thresh).astype(int)
    f1 = f1_score(val_labels, preds)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = thresh

print(f"Optimal threshold: {best_threshold:.2f}")

# Final evaluation with optimal threshold
test_ds = image_dataset_from_directory(
    f'{DATA_DIR}/test',
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='binary'
)

test_preds = model.predict(test_ds)
test_preds = (test_preds > best_threshold).astype(int)
test_labels = np.concatenate([y for x, y in test_ds], axis=0)

print("\nClassification Report:")
print(classification_report(test_labels, test_preds))
print("\nConfusion Matrix:")
print(confusion_matrix(test_labels, test_preds))

def plot_training_history(history):
    plt.figure(figsize=(18, 6))

    # Plot Loss
    plt.subplot(1, 3, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Loss Curve\nLower is Better', pad=20)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.grid(True)

    # Plot Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(history.history['auc'], label='Train AUC')
    plt.plot(history.history['val_auc'], label='Val AUC')
    plt.title('AUC Curve\nHigher is Better', pad=20)
    plt.ylabel('AUC')
    plt.xlabel('Epoch')
    plt.legend()
    plt.grid(True)

    # Plot Precision-Recall
    plt.subplot(1, 3, 3)
    plt.plot(history.history['precision'], label='Train Precision')
    plt.plot(history.history['recall'], label='Train Recall')
    plt.plot(history.history['val_precision'], label='Val Precision')
    plt.plot(history.history['val_recall'], label='Val Recall')
    plt.title('Precision-Recall Curves', pad=20)
    plt.ylabel('Score')
    plt.xlabel('Epoch')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# For phase 1 (head training)
plot_training_history(history)

# For phase 2 (fine-tuning)
plot_training_history(history_fine)

# Combined loss visualization
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'] + history_fine.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'] + history_fine.history['val_loss'], label='Val Loss')
plt.title('Combined Training Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.grid(True)
plt.show()

