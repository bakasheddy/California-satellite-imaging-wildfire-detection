# -*- coding: utf-8 -*-
"""Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mVgArktT1hsKygE5KAmnF3tX8kSVIEUF
"""

import os
import cv2
import numpy as np
import shutil
import matplotlib.pyplot as plt
from tqdm import tqdm
import tensorflow as tf
from google.colab.patches import cv2_imshow
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dropout
from tensorflow.keras.layers import concatenate, Conv2DTranspose
from tensorflow.keras.metrics import MeanIoU

# ========================
# Configuration
# ========================
DATASET_ROOT = "/content/drive/MyDrive/FIRMS_Data"
NEW_SIZE = (512, 512)  # Standard size for processing
DEBUG_MODE = True  # Set to False for batch processing

# Color thresholds (optimized for fire detection)
HSV_LOWER = np.array([0, 50, 50])
HSV_UPPER = np.array([20, 255, 255])
LAB_LOWER = np.array([0, 130, 70])
LAB_UPPER = np.array([255, 145, 90])

# ========================
# Core Functions
# ========================

def enhanced_fire_mask(img_path):
    """Generate improved fire mask with validation checks"""
    try:
        # Load and resize image
        img = cv2.imread(img_path)
        if img is None:
            raise ValueError(f"Could not read image: {img_path}")

        img = cv2.resize(img, NEW_SIZE)

        # Multi-color space analysis
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

        # Adaptive thresholding
        mask_hsv = cv2.inRange(hsv, HSV_LOWER, HSV_UPPER)
        mask_lab = cv2.inRange(lab, LAB_LOWER, LAB_UPPER)
        combined = cv2.bitwise_or(mask_hsv, mask_lab)

        # Morphological refinement
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7,7))
        refined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel, iterations=3)
        refined = cv2.morphologyEx(refined, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))

        # Size-based filtering
        contours, _ = cv2.findContours(refined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for cnt in contours:
            if cv2.contourArea(cnt) < 50:  # Remove small detections
                cv2.drawContours(refined, [cnt], -1, 0, -1)

        return refined

    except Exception as e:
        print(f"Error processing {img_path}: {str(e)}")
        return np.zeros(NEW_SIZE[::-1], dtype=np.uint8)

def visualize_sample(image_path, mask_path):
    """Quality control visualization"""
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    plt.figure(figsize=(15,5))
    plt.subplot(131), plt.imshow(img), plt.title('Original Image')
    plt.subplot(132), plt.imshow(mask, cmap='hot'), plt.title('Fire Mask')
    plt.subplot(133), plt.imshow(cv2.bitwise_and(img, img, mask=mask)), plt.title('Overlay')
    plt.show()

# ========================
# Dataset Restructuring
# ========================

def restructure_dataset():
    """Convert classification dataset to segmentation-ready format"""
    for split in ['train', 'val', 'test']:
        print(f"\n{'='*40}\nProcessing {split} set\n{'='*40}")

        # Create new directories
        img_dir = os.path.join(DATASET_ROOT, split, 'images')
        mask_dir = os.path.join(DATASET_ROOT, split, 'masks')
        os.makedirs(img_dir, exist_ok=True)
        os.makedirs(mask_dir, exist_ok=True)

        # Process fire images
        fire_src = os.path.join(DATASET_ROOT, split, 'fire')
        if os.path.exists(fire_src):
            for fname in tqdm(os.listdir(fire_src), desc="Fire images"):
                if not fname.lower().endswith(('.png', '.jpg', '.jpeg')):
                    continue

                src_path = os.path.join(fire_src, fname)
                dest_img = os.path.join(img_dir, fname)
                dest_mask = os.path.join(mask_dir, f"mask_{fname}")

                # Generate and save mask
                mask = enhanced_fire_mask(src_path)
                cv2.imwrite(dest_mask, mask)

                # Save resized image
                img = cv2.imread(src_path)
                img = cv2.resize(img, NEW_SIZE)
                cv2.imwrite(dest_img, img)

                if DEBUG_MODE:
                    visualize_sample(dest_img, dest_mask)

        # Process no-fire images (empty masks)
        nofire_src = os.path.join(DATASET_ROOT, split, 'no_fire')
        if os.path.exists(nofire_src):
            for fname in tqdm(os.listdir(nofire_src), desc="No-fire images"):
                if not fname.lower().endswith(('.png', '.jpg', '.jpeg')):
                    continue

                src_path = os.path.join(nofire_src, fname)
                dest_img = os.path.join(img_dir, fname)
                dest_mask = os.path.join(mask_dir, f"mask_{fname}")

                # Save empty mask
                cv2.imwrite(dest_mask, np.zeros(NEW_SIZE[::-1], dtype=np.uint8))

                # Save resized image
                img = cv2.imread(src_path)
                img = cv2.resize(img, NEW_SIZE)
                cv2.imwrite(dest_img, img)

# ========================
# Dataset Verification
# ========================

def verify_dataset():
    """Validate the restructured dataset"""
    for split in ['train', 'val', 'test']:
        img_dir = os.path.join(DATASET_ROOT, split, 'images')
        mask_dir = os.path.join(DATASET_ROOT, split, 'masks')

        images = set(os.listdir(img_dir))
        masks = set(os.listdir(mask_dir))

        # Check filename correspondence
        expected_masks = {f"mask_{fname}" for fname in images}
        missing_masks = expected_masks - masks
        extra_masks = masks - expected_masks

        print(f"\n{split.upper()} Verification:")
        print(f"Images: {len(images)}, Masks: {len(masks)}")
        print(f"Missing masks: {len(missing_masks)}")
        print(f"Extra masks: {len(extra_masks)}")

        # Spot check masks
        sample_img = next(iter(images), None)
        if sample_img:
            sample_mask = f"mask_{sample_img}"
            visualize_sample(
                os.path.join(img_dir, sample_img),
                os.path.join(mask_dir, sample_mask)
            )

# ========================
# Execution
# ========================
if __name__ == "__main__":
    restructure_dataset()
    verify_dataset()

# Configuration
BATCH_SIZE = 8
IMG_SIZE = (512, 512)
DATASET_PATH = "/content/drive/MyDrive/FIRMS_Data"

def parse_image_mask_pair(image_path, mask_path):
    # Load and normalize image
    image = tf.io.read_file(image_path)
    image = tf.image.decode_png(image, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)

    # Load and normalize mask
    mask = tf.io.read_file(mask_path)
    mask = tf.image.decode_png(mask, channels=1)
    mask = tf.image.convert_image_dtype(mask, tf.float32)
    mask = tf.where(mask > 0.5, 1.0, 0.0)  # Ensure binary mask

    return image, mask

def create_dataset(split):
    image_dir = f"{DATASET_PATH}/{split}/images"
    mask_dir = f"{DATASET_PATH}/{split}/masks"

    image_paths = tf.data.Dataset.list_files(f"{image_dir}/*", shuffle=False)
    mask_paths = tf.data.Dataset.list_files(f"{mask_dir}/*", shuffle=False)

    dataset = tf.data.Dataset.zip((image_paths, mask_paths))
    dataset = dataset.map(parse_image_mask_pair, num_parallel_calls=tf.data.AUTOTUNE)

    if split == 'train':
        # Augmentation pipeline
        dataset = dataset.map(lambda x, y: (
            tf.image.random_flip_left_right(x),
            tf.image.random_flip_left_right(y)
        ), num_parallel_calls=tf.data.AUTOTUNE)

        dataset = dataset.map(lambda x, y: (
            tf.image.random_brightness(x, 0.1),
            y
        ), num_parallel_calls=tf.data.AUTOTUNE)

        dataset = dataset.shuffle(100)

    return dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Create datasets
train_ds = create_dataset('train')
val_ds = create_dataset('val')
test_ds = create_dataset('test')

def unet_model(input_size=(512, 512, 3)):
    inputs = Input(input_size)

    # Encoder (Downsampling)
    # Block 1
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)
    pool1 = MaxPool2D(2)(conv1)

    # Block 2
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)
    pool2 = MaxPool2D(2)(conv2)

    # Block 3
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)
    pool3 = MaxPool2D(2)(conv3)

    # Middle
    convm = Conv2D(512, 3, activation='relu', padding='same')(pool3)
    convm = Conv2D(512, 3, activation='relu', padding='same')(convm)

    # Decoder (Upsampling)
    # Block 4
    up4 = Conv2DTranspose(256, 2, strides=2, padding='same')(convm)
    concat4 = concatenate([up4, conv3])
    conv4 = Conv2D(256, 3, activation='relu', padding='same')(concat4)
    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)

    # Block 5
    up5 = Conv2DTranspose(128, 2, strides=2, padding='same')(conv4)
    concat5 = concatenate([up5, conv2])
    conv5 = Conv2D(128, 3, activation='relu', padding='same')(concat5)
    conv5 = Conv2D(128, 3, activation='relu', padding='same')(conv5)

    # Block 6
    up6 = Conv2DTranspose(64, 2, strides=2, padding='same')(conv5)
    concat6 = concatenate([up6, conv1])
    conv6 = Conv2D(64, 3, activation='relu', padding='same')(concat6)
    conv6 = Conv2D(64, 3, activation='relu', padding='same')(conv6)

    # Output
    outputs = Conv2D(1, 1, activation='sigmoid')(conv6)

    return tf.keras.Model(inputs, outputs)

model = unet_model()
model.summary()

def dice_coeff(y_true, y_pred, smooth=1e-6):
    y_true = tf.cast(y_true, tf.float32)
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)
    return (2. * intersection + smooth) / (union + smooth)

def dice_loss(y_true, y_pred):
    return 1 - dice_coeff(y_true, y_pred)

model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
              loss=dice_loss,
              metrics=['accuracy', MeanIoU(num_classes=2)])

callbacks = [
    tf.keras.callbacks.ModelCheckpoint('best_wildfire_model.h5',
                                      save_best_only=True,
                                      monitor='val_loss'),
    tf.keras.callbacks.EarlyStopping(patience=10,
                                    restore_best_weights=True)
]

history = model.fit(train_ds,
                    validation_data=val_ds,
                    epochs=50,
                    callbacks=callbacks)

# Plot training history
def plot_history(history):
    plt.figure(figsize=(15,5))

    plt.subplot(1,2,1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Loss Curve')
    plt.legend()

    plt.subplot(1,2,2)
    plt.plot(history.history['mean_io_u'], label='Train IoU')
    plt.plot(history.history['val_mean_io_u'], label='Val IoU')
    plt.title('IoU Metric')
    plt.legend()

    plt.show()

plot_history(history)

# Test set evaluation
test_results = model.evaluate(test_ds)
print(f"Test Loss: {test_results[0]:.4f}")
print(f"Test Accuracy: {test_results[1]:.4f}")
print(f"Test IoU: {test_results[2]:.4f}")

# Prediction visualization
def show_predictions(dataset, num=3):
    for images, masks in dataset.take(1):
        preds = model.predict(images)
        plt.figure(figsize=(15, num*5))
        for i in range(num):
            plt.subplot(num,3,i*3+1)
            plt.imshow(images[i])
            plt.title('Input Image')

            plt.subplot(num,3,i*3+2)
            plt.imshow(masks[i], cmap='gray')
            plt.title('True Mask')

            plt.subplot(num,3,i*3+3)
            plt.imshow(preds[i] > 0.5, cmap='gray')
            plt.title('Predicted Mask')
        plt.show()

show_predictions(test_ds)